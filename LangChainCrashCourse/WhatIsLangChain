# https://www.youtube.com/watch?v=at6XrRRdOQo&list=LL&index=2&t=41s
# https://github.com/harishneel1/langchain-course
# https://platform.openai.com/settings/organization/billing/overview

# Prompt in chatGpt - I want to plan a trip to Paris this Saturday. Can you book my flight, also book a hotel the same day, and suggest some good restaurants?
# ChatGpt itself an app with UI -> query(above prompt) is now sent to LLM (GPT-3.5-turbo, GPT-4o, etc) -> LLM processes the prompt and returns the output -> output is displayed in the UI
# Response in short - I can't book flights or hotels directly, but I can definitely help you plan your trip! Here are suggestions ......
(cant book tickets - biggest limitation of LLM)
# LLMs are smart and can talk about travel but they cannot actually interact with real world, so on its own LLMs are just the brains, they can be trained on certain data
# and it will reason with it but it cannot do anything outside of it.
# Reasoning ability of LLMs (brain), but at the same time it should have the ability to communicate with the real world apis/dbs/send emails.
# If you wanna do that we need to have some sort of a framework that is going to help us to connect the LLM to the real world.
# LangChain acts a bridge between LLM and real world. LangChain the most popular framework that helps build apps using LLMs.

# In the future if you want to switch out from GPT 4o with lets say a free HuggingFace LLM if you are short on cash or maybe you want to use a different LLM like Claude Sonnet.
# You can do so without even touching the code that you wrote with LangChain.
# So with LangChain the AI that we are working with can do so much with real world.
# example: it can access lot of apis, like it can access flight and restaurant booking api, booking.com, access private company dbs to answer customer queries, it can send emails, it can browse google, wikipedia, it can scape websites and a lot lot more..
# LangChain doesnt just make the AI smarter but it gives the ability to act in the real world.

# python -m venv .venv | go to new terminal and verify the (.venv) is added to the path.
# -m - module
# venv - module name
# .venv - virtual environment name
In settings - python interpreter -> dropdown -> show all -> add local interpreter - venv - apply - ok

# 1st core component of the LangChain is chat models (https://python.langchain.com/docs/integrations/chat/)
# A chat model in LangChain is a component designed to communicate in a structured way with the LLMs like GPT-4, Hugging Face, and claude sonnet.

Why use LangChain Chat models?
1. Consistent workflow:
    LangChains Chat models unify different APIs, saving you from managing each one's unique setup and quirks.
2. Easy switching between LLMs
    Want to switch from one LLM to another? LangChain's chat models make it simple without code rewrites.
3. Context management:
    LangChain's chat models manage the context of the conversation, making it easy to keep track of the conversation history and state.
4. Efficient Chaining
    You can connect multiple LLM calls and tasks in one structured pipeline, which is tricky to setup manually.
5. Scalability:
    As projects grow, LangChain's interface supports more complex workflows, letting you focus on features, not API management.

# pip install python-dotenv
# pip install -qU langchain-ollama (or) pip install -qU langchain-openai (or) pip install -qU langchain-google-genai (or) pip install -qU langchain-anthropic
# -q - quiet mode
# -U - upgrade the package if it is already installed to the latest version.
# pip install -qU langchain-google-firestore

Types of Messages in LangChain
1. SystemMessage: Defines the AI's role and sets the context for the conversation
Example. "You are a marketing expert."
2. HumanMessage: Represents user input or questions directed to the AI
Example. "What's a good marketing strategy"?
3. AIMessage: Contains the AI's responses based on previous messages
Example. "Focus on social media engagement"

# 2nd core component of the LangChain is Prompt templates, which are a way to define a set of prompts that are going to be used by the AI.
# In production grade applications, you will be using it a lot.

# 3rd core component of the LangChain is the Chains. It lets you connect multiple tasks together by chaining it together and it helps you create sort of like an unified workflow.
.It is a sequence of steps that the input goes through before it reaches the model.

# TYPES OF CHAINING
1. Extended or Sequential Chaining:
Chaining tasks one by one is a straight/sequential line
2. Parallel Chaining:
Let you run tasks parallely or simultaneously without being dependent on each other
3. Conditional Chaining:
Let you run a particular branch based on a condition

# 4th core component of the LangChain is RAG. It is what is creating a lot of impact in how businesses operate and increase efficiency.
Retrieval Augmented Generation (RAG).

# RAGs solves one problem with LLM and the problem it solves:
RAGs give LLMs additional knowledge
In other words, we use RAGs to provide LLMs an external source of information to give better answers to out prompts.

# Real-world example:
Let's say you're working at a company with 1OOs of internal documents ( like policy guidelines, technical specs, customer support documents, etc)

Now, if you have a question that could be answered by one of these documents, you would normally need to search through each file manually

But with RAGs, this process can be streamlined. (You can give access to all these private documents to the LLM or provide external source of information to LLM)

So the next time when you have a question, you can just ask it in simple English and the LLM retrieves the most relevant info from those documents and uses it to give you an accurate answer—all in one go.

# Next challenge solved by RAG: Context Window Limitation - if you give large huge prompts to LLM, it really struggles to answer the question, it sort of starts hallucinating and it sort of starts to forget what happened before that in the past and all of that because of the context window or the limit becomes very high.
It solves this problem by only pulling/retrieving the relevant sections of the documents based on the user prompt.

RAGs is a method where we combine LLMs with a retrieval system.
This retrieval system can search through vast sources of external information—like documents, databases, or knowledge bases—whenever the LLM needs additional knowledge to give you a better answers. All the while making sure LLMs are not overwhelmed with bigger prompts.

What are tokens?
In the context of language models, a token is a unit of text that the model processes.
Tokens can be as short as one character or as long as one word, depending on the language and structure of the text.
For example, the word "hello" is one token, while the phrase "I'm" is typically broken down into two tokens: "I" and '"m."

Tokens are crucial because LLMs have a limit on how many tokens they can handle at once, often referred to as the "context window."
This means that if we have a PDF that's 10 million tokens long, we can't feed it to the model in one go.

# Chunking - is a process of breaking down a large document into smaller, more manageable pieces.
# Retriever - is a component that searches through these chunks to find the most relevant information based on the user's prompt and pass it to the model along with the user prompt.

# Embeddings and Vector DBs
What are embeddings?
A vector embedding is a mathematical representation of words, sentences or even images.
For example, the word "cat" can have a vector embedding that can look something like this: [34, 21, 7.5, -12, 0.2, 18, -3]
So this is the computer's way of making sense of the word "cat".
Basically each of these numbers can represent a certain aspect of the word "cat".
(picture this in a x,y axis graph) Each of the values are actual coordinates in a multi dimensional space, so the word "cat"/"Kitten" is represented by a point in this multi dimensional space.

Embeddings and Vector DBs
Vector DB definition:
======================
A database built to store all the vector embeddings is considered a vector database.
What does it do? - basically lets us retrieve information or update info or do something, vector dbs as well enables us to retrieve info based on **meaning** rather than on exact word matches.
In vector store, we store the actual plain text of the document and the embeddings of the document. Each chunk in the vector store is going to have the embedded representation and plain text version as well.

We can also think RAGs as giving long-term memory to LLMs.
Example: There is a customer support bot, so all the past conversations with a specific user can be stored in a vector db so that in the future if there's a specific question
from history that the user asked, the bot can refer to the past conversation and give a more accurate answer.

# Dependencies to install
pip install chromadb - This will contain all the core modules needed to create the vector store.
pip install langchain-chroma - This will be a wrapper that connects our langchain application with the vector store.

# Chunk overlap?
Reason #1
Chunk Overlap defines how many characters (or tokens, depending on the splitter) should be included in both the end of one chunk and the beginning of the next chunk.
By setting Chunk Overlap = O, you are telling the splitter to ensure that there's no overlap between chunks
If you set chunk_overlap = 100, for example, the last 100 characters of one chunk will appear at the beginning of the next chunk. This helps preserve context across chunks.

Reason #2: Semantic Understanding
Especially when working with large texts (like books), chunking with overlap can help with better understanding across the chunks when querying or processing.

# RAGs - With MetaData
Basically being able to add more information about where the chunk came from, for example this chunk came from this book this chapter this passage, etc.
So that in future, by retrieving it we'll actually be able to see where the source of chunk came from

# Why do we need it?
Let's say we've put a 100 different private docs or books in our vector store, and we ask a question.
In addition to getting the chunks, it'll be better if we also know where the chunk originated from right?, For ex, like it's this book, chapter 4, paragraph 3 or this
document, this section, paragraph 7


# AI Agents & Tools - Basic
Giving your AI the ability to actually use the tools and make decision like a actual human being.

Think of Agents as the "problem-solvers" of the AI world. Agents are capable of thinking on their own.
In other words, it's AI that can make autonomous decisions.
** In the case of Chains and RAGs, they follow our specific instruction **.
But Agents, they actually take it a step further. They can decide for themselves what steps to take on their own.

# What exactly are agents?
Think of Agents as AI decision-makers that can pick the right tool for the job without being told what to use.
(The phrase "without being told what to use" means that the AI agents can autonomously decide which tool or method is best suited for a given task. They don't need explicit instructions or guidance on which tool to select; they can make that decision on their own based on the context and requirements.)

Just like a chef who knows when to use a knife, whisk, or oven to create a perfect dish, an AI can decide when it should browse the internet, or
when to use a calculator or when to make an API call to your database.

# What are tools then?
Tools are specific functions that Agents can use to complete tasks.

Just like a chef's kitchen tools (knife for cutting, oven for baking, blender formixing), tools are the special abilities we give to AI - like giving it a calculator
tool, or a search engine tool, or a calendar tool.

Agents help language models decide what actions to take when solving a problem.
They choose the right tools at the right time without us telling them which tool to use.

# Example to understand:
If I ask an AI Agent "What is the weather in Paris + 5", it gets really interesting.
The AI Agent isn't just randomly picking tools - it's actually thinking through the process, kind of like a detective solving a case.
Firstly, the Agent saw two parts to the problem: finding weather AND doing math.
It knows to get the weather FIRST (because you can't add 5 to something you don't know yet).
Then it uses the calculator tool to add 5.

But you might ask, how does the Agent know what to do first and what to do second?
This is where the concept of reACT comes in.
This is one of the best known patterns in AI today to build agents. And by ReACt I don't mean the frontend framework. We're talking in terms of AI terms. It stands
for Reasoning + Acting.
This is basically a concept that mimics how human beings think.

ReACT pattern
Think/reasoning: It first thinks / LLM first thinks about the user prompt/problem
Act: The agent can act or perform the task using various tools available / LLM decides if it can answer by itself or if it should use a tool
Act Input: LLM provides the input argument for the tool (python function)
    (Here,(::Observation::) LangChain executes the tool and returns the output to the LLM)
Observe: Observe what happened. / LLM observe the result of the tool
(Repeat the entire process above if needed).
.
.
.
Final answer: "This is your final answer"

Example of a complicated prompt:
"Should I pack an umbrella for my trip to London next week, and what restaurants should I book?"
Now this is a multi-step problem and our AI-Agent might loop through thought-action-observation-repeat-thought-action-observation multiple times to
solve this problem.

# pip install langchainhub - This is a package that contains all the tools and agents that you need to build a AI agent.
It lets us use other people's prompt templates from the internet. So instead of writing my own I'm going to use someone else's.

----------------------------------------------------------------------------------------
# LangChain Memory (https://www.youtube.com/watch?v=sYlMD2OFEgc)
Why is Memory Important?
People treat Chat Agents like they're human
So there needs to be some way for the LLM to have memory of past conversations

# Existing attempts to add memory to LLMs:
In the past, developers have tried to add in-built memory to transformer models (memory into the LLM itself)
But so far, it hasn't been very optimal
But we can expect it to be added to models as models become more powerful

# What options do we have right now for memory in LLMs?
1.	Put it as part of the prompt (Whenever you are asking a question to the LLM,
there could have been several back and forth conversation happened in the past, you can add all of that history to the LLM in the prompt)
Disadvantages:
Content window limitation
Latest OpenAI models only allow 124,000 tokens per prompt

2.	External look-up (Using RAGs to look up information in a database)

# How does LangChain Memory solves it?
Simple Classes:
ConversationBufferMemory
ConversationSummaryMemory

Advanced Classes:
ConversationBufferMemoryWindow
ConversationSummaryBufferMemory
EntityMemory
CustomMemory

python.langchain.com/v0.1/docs/modules/memory/types/buffer/
-----------------------------------------------------------------------------------------------







