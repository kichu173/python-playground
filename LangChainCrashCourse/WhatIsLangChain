# https://www.youtube.com/watch?v=at6XrRRdOQo&list=LL&index=2&t=41s
# https://github.com/harishneel1/langchain-course
# https://platform.openai.com/settings/organization/billing/overview

# Prompt in chatGpt - I want to plan a trip to Paris this Saturday. Can you book my flight, also book a hotel the same day, and suggest some good restaurants?
# ChatGpt itself an app with UI -> query(above prompt) is now sent to LLM (GPT-3.5-turbo, GPT-4o, etc) -> LLM processes the prompt and returns the output -> output is displayed in the UI
# Response in short - I can't book flights or hotels directly, but I can definitely help you plan your trip! Here are suggestions ......
(cant book tickets - biggest limitation of LLM)
# LLMs are smart and can talk about travel but they cannot actually interact with real world, so on its own LLMs are just the brains, they can be trained on certain data
# and it will reason with it but it cannot do anything outside of it.
# Reasoning ability of LLMs (brain), but at the same time it should have the ability to communicate with the real world apis/dbs/send emails.
# If you wanna do that we need to have some sort of a framework that is going to help us to connect the LLM to the real world.
# LangChain acts a bridge between LLM and real world. LangChain the most popular framework that helps build apps using LLMs.

# In the future if you want to switch out from GPT 4o with lets say a free HuggingFace LLM if you are short on cash or maybe you want to use a different LLM like Claude Sonnet.
# You can do so without even touching the code that you wrote with LangChain.
# So with LangChain the AI that we are working with can do so much with real world.
# example: it can access lot of apis, like it can access flight and restaurant booking api, booking.com, access private company dbs to answer customer queries, it can send emails, it can browse google, wikipedia, it can scape websites and a lot lot more..
# LangChain doesnt just make the AI smarter but it gives the ability to act in the real world.

# python -m venv .venv
# -m - module
# venv - module name
# .venv - virtual environment name
In settings - python interpreter -> dropdown -> show all -> add local interpreter - venv - apply - ok

# 1st core component of the LangChain is chat models (https://python.langchain.com/docs/integrations/chat/)
# A chat model in LangChain is a component designed to communicate in a structured way with the LLMs like GPT-4, Hugging Face, and claude sonnet.

Why use LangChain Chat models?
1. Consistent workflow:
    LangChains Chat models unify different APIs, saving you from managing each one's unique setup and quirks.
2. Easy switching between LLMs
    Want to switch from one LLM to another? LangChain's chat models make it simple without code rewrites.
3. Context management:
    LangChain's chat models manage the context of the conversation, making it easy to keep track of the conversation history and state.
4. Efficient Chaining
    You can connect multiple LLM calls and tasks in one structured pipeline, which is tricky to setup manually.
5. Scalability:
    As projects grow, LangChain's interface supports more complex workflows, letting you focus on features, not API management.

# pip install python-dotenv
# pip install -qU langchain-ollama (or) pip install -qU langchain-openai (or) pip install -qU langchain-google-genai (or) pip install -qU langchain-anthropic
# -q - quiet mode
# -U - upgrade the package if it is already installed to the latest version.
# pip install -qU langchain-google-firestore

Types of Messages in LangChain
1. SystemMessage: Defines the AI's role and sets the context for the conversation
Example. "You are a marketing expert."
2. HumanMessage: Represents user input or questions directed to the AI
Example. "What's a good marketing strategy"?
3. AIMessage: Contains the AI's responses based on previous messages
Example. "Focus on social media engagement"

# 2nd core component of the LangChain is Prompt templates, which are a way to define a set of prompts that are going to be used by the AI.
# In production grade applications, you will be using it a lot.

# 3rd core component of the LangChain is the Chains. It lets you connect multiple tasks together by chaining it together and it helps you create sort of like an unified workflow.
.It is a sequence of steps that the input goes through before it reaches the model.

# TYPES OF CHAINING
1. Extended or Sequential Chaining:
Chaining tasks one by one is a straight/sequential line
2. Parallel Chaining:
Let you run tasks parallely or simultaneously without being dependent on each other
3. Conditional Chaining:
Let you run a particular branch based on a condition

# 4th core component of the LangChain is RAG. It is what is creating a lot of impact in how businesses operate and increase efficiency.
Retrieval Augmented Generation (RAG).

# RAGs solves one problem with LLM and the problem it solves:
RAGs give LLMs additional knowledge
In other words, we use RAGs to provide LLMs an external source of information to give better answers to out prompts.

# Real-world example:
Let's say you're working at a company with 1OOs of internal documents ( like policy guidelines, technical specs, customer support documents, etc)

Now, if you have a question that could be answered by one of these documents, you would normally need to search through each file manually

But with RAGs, this process can be streamlined. (You can give access to all these private documents to the LLM or provide external source of information to LLM)

So the next time when you have a question, you can just ask it in simple English and the LLM retrieves the most relevant info from those documents and uses it to give you an accurate answer—all in one go.

# Next challenge solved by RAG: Context Window Limitation - if you give large huge prompts to LLM, it really struggles to answer the question, it sort of starts hallucinating and it sort of starts to forget what happened before that in the past and all of that because of the context window or the limit becomes very high.
It solves this problem by only pulling/retrieving the relevant sections of the documents based on the user prompt.

RAGs is a method where we combine LLMs with a retrieval system.
This retrieval system can search through vast sources of external information—like documents, databases, or knowledge bases—whenever the LLM needs additional knowledge to give you a better answers. All the while making sure LLMs are not overwhelmed with bigger prompts.

What are tokens?
In the context of language models, a token is a unit of text that the model processes.
Tokens can be as short as one character or as long as one word, depending on the language and structure of the text.
For example, the word "hello" is one token, while the phrase "I'm" is typically broken down into two tokens: "I" and '"m."

Tokens are crucial because LLMs have a limit on how many tokens they can handle at once, often referred to as the "context window."
This means that if we have a PDF that's 10 million tokens long, we can't feed it to the model in one go.

# Chunking - is a process of breaking down a large document into smaller, more manageable pieces.
# Retriever - is a component that searches through these chunks to find the most relevant information based on the user's prompt and pass it to the model along with the user prompt.

# Embeddings and Vector DBs
What are embeddings?
A vector embedding is a mathematical representation of words, sentences or even images.
For example, the word "cat" can have a vector embedding that can look something like this: [34, 21, 7.5, -12, 0.2, 18, -3]
So this is the computer's way of making sense of the word "cat".
Basically each of these numbers can represent a certain aspect of the word "cat".

Embeddings and Vector DBs
Vector DB definition:
======================
A database built to store all the vector embeddings is considered a vector database.
What does it do? - basically lets us retrieve information or update info or do something, vector dbs as well enables us to retrieve info based on meaning rather than on exact word matches.
In vector store, we store the actual plain text of the document and the embeddings of the document.

We can also think RAGs as giving long-term memory to LLMs.
Example: There is a customer support bot, so all the past conversations with a specific user can be stored in a vector db so that in the future if there's a specific question
from history that the user asked, the bot can refer to the past conversation and give a more accurate answer.

# Dependencies to install
pip install chromadb - This will contain all the core modules needed to create the vector store.
pip install langchain-chroma - This will be a wrapper that connects our langchain application with the vector store.

# Chunk overlap?
Reason #1
Chunk Overlap defines how many characters (or tokens, depending on the splitter) should be included in both the end of one chunk and the beginning of the next chunk.
By setting Chunk Overlap = O, you are telling the splitter to ensure that there's no overlap between chunks
If you set chunk_overlap = 100, for example, the last 100 characters of one chunk will appear at the beginning of the next chunk. This helps preserve context across chunks.

Reason #2: Semantic Understanding
Especially when working with large texts (like books), chunking with overlap can help with better understanding across the chunks when querying or processing.
