# https://www.youtube.com/watch?v=at6XrRRdOQo&list=LL&index=2&t=41s
# https://github.com/harishneel1/langchain-course
# https://platform.openai.com/settings/organization/billing/overview

# Prompt in chatGpt - I want to plan a trip to Paris this Saturday. Can you book my flight, also book a hotel the same day, and suggest some good restaurants?
# ChatGpt itself an app with UI -> query(above prompt) is now sent to LLM (GPT-3.5-turbo, GPT-4o, etc) -> LLM processes the prompt and returns the output -> output is displayed in the UI
# Response in short - I can't book flights or hotels directly, but I can definitely help you plan your trip! Here are suggestions ......
(cant book tickets - biggest limitation of LLM)
# LLMs are smart and can talk about travel but they cannot actually interact with real world, so on its own LLMs are just the brains, they can be trained on certain data
# and it will reason with it but it cannot do anything outside of it.
# Reasoning ability of LLMs (brain), but at the same time it should have the ability to communicate with the real world apis/dbs/send emails.
# If you wanna do that we need to have some sort of a framework that is going to help us to connect the LLM to the real world.
# LangChain acts a bridge between LLM and real world. LangChain the most popular framework that helps build apps using LLMs.

# In the future if you want to switch out from GPT 4o with lets say a free HuggingFace LLM if you are short on cash or maybe you want to use a different LLM like Claude Sonnet.
# You can do so without even touching the code that you wrote with LangChain.
# So with LangChain the AI that we are working with can do so much with real world.
# example: it can access lot of apis, like it can access flight and restaurant booking api, booking.com, access private company dbs to answer customer queries, it can send emails, it can browse google, wikipedia, it can scape websites and a lot lot more..
# LangChain doesnt just make the AI smarter but it gives the ability to act in the real world.

# python -m venv .venv
# -m - module
# venv - module name
# .venv - virtual environment name
In settings - python interpreter -> dropdown -> show all -> add local interpreter - venv - apply - ok

# 1st core component of the LangChain is chat models (https://python.langchain.com/docs/integrations/chat/)
# A chat model in LangChain is a component designed to communicate in a structured way with the LLMs like GPT-4, Hugging Face, and claude sonnet.

Why use LangChain Chat models?
1. Consistent workflow:
    LangChains Chat models unify different APIs, saving you from managing each one's unique setup and quirks.
2. Easy switching between LLMs
    Want to switch from one LLM to another? LangChain's chat models make it simple without code rewrites.
3. Context management:
    LangChain's chat models manage the context of the conversation, making it easy to keep track of the conversation history and state.
4. Efficient Chaining
    You can connect multiple LLM calls and tasks in one structured pipeline, which is tricky to setup manually.
5. Scalability:
    As projects grow, LangChain's interface supports more complex workflows, letting you focus on features, not API management.

# pip install python-dotenv
# pip install -qU langchain-ollama (or) pip install -qU langchain-openai (or) pip install -qU langchain-google-genai (or) pip install -qU langchain-anthropic
# -q - quiet mode
# -U - upgrade the package if it is already installed to the latest version.
# pip install -qU langchain-google-firestore

Types of Messages in LangChain
1. SystemMessage: Defines the AI's role and sets the context for the conversation
Example. "You are a marketing expert."
2. HumanMessage: Represents user input or questions directed to the AI
Example. "What's a good marketing strategy"?
3. AIMessage: Contains the AI's responses based on previous messages
Example. "Focus on social media engagement"

# 2nd core component of the LangChain is Prompt templates, which are a way to define a set of prompts that are going to be used by the AI.
# In production grade applications, you will be using it a lot.

# 3rd core component of the LangChain is the Chains. It lets you connect multiple tasks together by chaining it together and it helps you create sort of like an unified workflow.
.It is a sequence of steps that the input goes through before it reaches the model.

# TYPES OF CHAINING
1. Extended or Sequential Chaining:
Chaining tasks one by one is a straight/sequential line
2. Parallel Chaining:
Let you run tasks parallely or simultaneously without being dependent on each other
3. Conditional Chaining:
Let you run a particular branch based on a condition

# 4th core component of the LangChain is RAG. It is what is creating a lot of impact in how businesses operate and increase efficiency.
Retrieval Augmented Generation (RAG).

# RAGs solves one problem with LLM and the problem it solves:
RAGs give LLMs additional knowledge
In other words, we use RAGs to provide LLMs an external source of information to give better answers to out prompts.

# Real-world example:
Let's say you're working at a company with IOOs of internal documents ( like policy guidelines, technical specs, customer support documents, etc)

Now, if you have a question that could be answered by one of these documents, you would normally need to search through each file manually

But with RAGs, this process can be streamlined. (You can give access to all these private documents to the LLM or provide external source of information to LLM)

So the next time when you have a question, you can just ask it in simple English and the LLM retrieves the most relevant info from those documents and uses it to give you an accurate answer—all in one go.

# Next challenge solved by RAG: Context Window Limitation
It solves this problem by only pulling/retrieving the relevant sections of the documents based on the user prompt.

RAGs is a method where we combine LLMs with a retrieval system.
This retrieval system can search through vast sources of external information—like documents, databases, or knowledge bases—whenever the LLM needs additional knowledge to give you a better answers. All the while making sure LLMs are not overwhelmed with bigger prompts.
